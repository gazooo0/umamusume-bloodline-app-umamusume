import streamlit as st
import pandas as pd
import unicodedata
import time
from bs4 import BeautifulSoup
import requests
import gspread
import json
import os
from oauth2client.service_account import ServiceAccountCredentials

# === å®šæ•°è¨­å®š ====
HEADERS = {"User-Agent": "Mozilla/5.0"}
SPREADSHEET_ID = "1wMkpbOvqveVBkJSR85mpZcnKThYSEmusmsl710SaRKw"
SHEET_NAME = "cache_UMA"

# === Google Sheets æ¥ç¶š ===
def connect_to_gspread():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    json_str = os.environ.get("GOOGLE_CREDENTIALS_JSON")
    if not json_str:
        st.error("ç’°å¢ƒå¤‰æ•° GOOGLE_CREDENTIALS_JSON ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.stop()

    credentials_dict = json.loads(json_str)  # replace ãªã—ã§èª­ã¿è¾¼ã‚€
    credentials_dict["private_key"] = credentials_dict["private_key"].replace("\\n", "\n")  # â†ã“ã“ã ã‘å¤‰æ›

    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
    client = gspread.authorize(credentials)
    return client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

# === å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸€æ‹¬å–å¾— ===
@st.cache_data(show_spinner=False)
def load_entire_cache():
    sheet = connect_to_gspread()
    return sheet.get_all_records()

# === ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¤œç´¢ï¼ˆæ—¢å­˜é–¢æ•°ã‚’ä¿®æ­£ï¼‰ ===
def load_cached_result(race_id, bloodline, full_cache=None):
    if full_cache is None:
        full_cache = load_entire_cache()

    matched_rows = [
        r for r in full_cache
        if str(r.get("race_id", "")) == str(race_id)
        and str(r.get("ã‚¦ãƒå¨˜è¡€çµ±", "")) == str(bloodline)
    ]

    if not matched_rows:
        return []  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡ã—

    # âœ… ä¸€è‡´ã™ã‚‹å…¨ã¦ã®è¡ŒãŒã€Œè©²å½“ãªã—ã€ãªã‚‰è©²å½“ãªã—ã¨ã¿ãªã™ï¼ˆç©ºæ–‡å­—ã‚‚å«ã‚€ï¼‰
    if all("è©²å½“ãªã—" in str(r.get("è©²å½“ç®‡æ‰€", "")) for r in matched_rows):
        return "è©²å½“ãªã—"

    # ãã‚Œä»¥å¤–ã¯é€šå¸¸å‡¦ç†
    results = []
    for r in matched_rows:
        if "è©²å½“ãªã—" not in str(r.get("è©²å½“ç®‡æ‰€", "")):
            results.append({
                "é¦¬å": r.get("é¦¬å", ""),
                "è©²å½“ç®‡æ‰€": r.get("è©²å½“ç®‡æ‰€", ""),
                "ç«¶é¦¬å ´": r.get("ç«¶é¦¬å ´", ""),
                "ãƒ¬ãƒ¼ã‚¹": r.get("ãƒ¬ãƒ¼ã‚¹", "")
            })
    return results


# === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰ ===
def save_cached_result(rows, race_id=None, bloodline=None):
    sheet = connect_to_gspread()
    headers = ["é¦¬å", "è©²å½“ç®‡æ‰€", "ç«¶é¦¬å ´", "ãƒ¬ãƒ¼ã‚¹", "ã‚¦ãƒå¨˜è¡€çµ±", "race_id"]
    existing = sheet.get_all_records()

    # === ğŸ§¹ race_id ã¨ è¡€çµ±ãŒä¸¡æ–¹ä¸€è‡´ã™ã‚‹è¡Œã ã‘å‰Šé™¤ ===
    delete_indices = []
    for i, r in enumerate(existing):
        if str(r.get("race_id", "")) == str(race_id) and str(r.get("ã‚¦ãƒå¨˜è¡€çµ±", "")) == str(bloodline):
            delete_indices.append(i + 2)  # +2 ã¯ãƒ˜ãƒƒãƒ€è¡Œ + 1-index

    if delete_indices:
        for i in sorted(delete_indices, reverse=True):
            sheet.delete_rows(i)
        time.sleep(1.2)

    # === ä¿å­˜å‡¦ç†ï¼ˆè©²å½“ãªã—å¯¾å¿œï¼‰
    if rows == "è©²å½“ãªã—":
        # è©²å½“ãªã—è¡Œã¨ã—ã¦ä¿å­˜
        row = {
            "é¦¬å": "",
            "è©²å½“ç®‡æ‰€": "è©²å½“ãªã—",
            "ç«¶é¦¬å ´": "",
            "ãƒ¬ãƒ¼ã‚¹": "",
            "ã‚¦ãƒå¨˜è¡€çµ±": bloodline,
            "race_id": race_id
        }
        sheet.append_row([row.get(h, "") for h in headers])
    elif isinstance(rows, list):
        for r in rows:
            r["ã‚¦ãƒå¨˜è¡€çµ±"] = bloodline
            r["race_id"] = race_id
            sheet.append_row([r.get(h, "") for h in headers])

# === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰ ===
def save_cached_result(rows, race_id=None, bloodline=None):
    sheet = connect_to_gspread()
    headers = ["é¦¬å", "è©²å½“ç®‡æ‰€", "ç«¶é¦¬å ´", "ãƒ¬ãƒ¼ã‚¹", "ã‚¦ãƒå¨˜è¡€çµ±", "race_id"]
    existing = sheet.get_all_records()

    # === ğŸ§¹ race_id ã¨ è¡€çµ±ãŒä¸¡æ–¹ä¸€è‡´ã™ã‚‹è¡Œã ã‘å‰Šé™¤ ===
    delete_indices = []
    for i, r in enumerate(existing):
        if str(r.get("race_id", "")) == str(race_id) and str(r.get("ã‚¦ãƒå¨˜è¡€çµ±", "")) == str(bloodline):
            delete_indices.append(i + 2)  # +2 ã¯ãƒ˜ãƒƒãƒ€è¡Œ + 1-index

    if delete_indices:
        for i in sorted(delete_indices, reverse=True):
            sheet.delete_rows(i)
        time.sleep(1.2)

    # === ğŸ“ æ›¸ãè¾¼ã¿ ===
    if not rows:
        dummy = {
            "é¦¬å": "ï¼ˆè©²å½“ãªã—ï¼‰",
            "è©²å½“ç®‡æ‰€": "è©²å½“ãªã—",
            "ç«¶é¦¬å ´": "",
            "ãƒ¬ãƒ¼ã‚¹": "",
            "ã‚¦ãƒå¨˜è¡€çµ±": bloodline or "",
            "race_id": race_id or ""
        }
        sheet.append_row([dummy[h] for h in headers])
    else:
        values = [[row.get(h, "") for h in headers] for row in rows]
        sheet.append_rows(values)

    time.sleep(1.2)

# === è¡€çµ±ä½ç½®ãƒ©ãƒ™ãƒ« ===
def generate_position_labels():
    def dfs(pos, depth, max_depth):
        if depth > max_depth: return []
        result = [pos]
        result += dfs(pos + "çˆ¶", depth + 1, max_depth)
        result += dfs(pos + "æ¯", depth + 1, max_depth)
        return result
    return dfs("", 0, 5)[1:]
POSITION_LABELS = generate_position_labels()

# === ã‚¦ãƒå¨˜è¡€çµ±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===
umamusume_df = pd.read_csv("umamusume.csv")
image_dict = dict(zip(umamusume_df["kettou"], umamusume_df["url"]))
name_to_kettou = dict(zip(umamusume_df["kettou"], umamusume_df["kettou"]))

# === å‡ºèµ°é¦¬ãƒªãƒ³ã‚¯å–å¾— ===
def get_horse_links(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    res = requests.get(url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    horse_links = {}
    tables = soup.find_all("table", class_="RaceTable01")
    for table in tables:
        for a in table.find_all("a", href=True):
            if "/horse/" in a["href"]:
                name = a.get_text(strip=True)
                full_url = "https://db.netkeiba.com" + a["href"]
                if len(name) >= 2 and name not in horse_links:
                    horse_links[name] = full_url
    return horse_links

# === è¡€çµ±å–å¾— ===
def get_pedigree_with_positions(horse_url):
    horse_id = horse_url.rstrip("/").split("/")[-1]
    ped_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    res = requests.get(ped_url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.find("table", class_="blood_table")
    if not table:
        return {}
    names = {}
    td_list = table.find_all("td")
    for i, td in enumerate(td_list[:len(POSITION_LABELS)]):
        label = POSITION_LABELS[i]
        a = td.find("a")
        if a and a.text.strip():
            names[label] = a.text.strip()
    return names

# === ãƒãƒƒãƒåˆ¤å®š ===
def match_pedigree(pedigree_dict, target_name):
    matched_positions = []
    target_normalized = unicodedata.normalize("NFKC", target_name).strip().lower()
    for pos, name in pedigree_dict.items():
        key = unicodedata.normalize("NFKC", name).strip().lower()
        if key == target_normalized:
            matched_positions.append(pos)
    return matched_positions

# === HTMLãƒ†ãƒ¼ãƒ–ãƒ«å¤‰æ›ï¼ˆè¡¨ç¤ºé …ç›®ã‚’åˆ¶é™ï¼‰ ===
def render_table_html(df):
    df = df[["é¦¬å", "è©²å½“ç®‡æ‰€", "ç«¶é¦¬å ´", "ãƒ¬ãƒ¼ã‚¹"]]  # è¡¨ç¤ºç”¨ã«åˆ—ã‚’é™å®š
    df.insert(0, "No", range(1, len(df) + 1))  # Noåˆ—è¿½åŠ 
    table_html = "<table border='1' style='border-collapse:collapse; width:100%; font-size:16px;'>"
    table_html += "<thead><tr>" + "".join([f"<th>{col}</th>" for col in df.columns]) + "</tr></thead><tbody>"
    for _, row in df.iterrows():
        table_html += "<tr>" + "".join([f"<td>{row[col]}</td>" for col in df.columns]) + "</tr>"
    table_html += "</tbody></table>"
    return table_html

# === Streamlit UI ===
st.title("ğŸ‘§ ã‚¦ãƒå¨˜é€†å¼•ãè¡€çµ±ã‚µãƒ¼ãƒ")

# JRAæ—¥ä»˜
schedule_df = pd.read_csv("jra_2025_keibabook_schedule.csv")
schedule_df["æ—¥ä»˜"] = pd.to_datetime(
    schedule_df["å¹´"].astype(str) + "/" + schedule_df["æœˆæ—¥(æ›œæ—¥)"].str.extract(r"(\d{2}/\d{2})")[0],
    format="%Y/%m/%d"
)
today = pd.Timestamp.today()
past_31 = today - pd.Timedelta(days=31)
future_7 = today + pd.Timedelta(days=7)
schedule_df = schedule_df[schedule_df["æ—¥ä»˜"].between(past_31, future_7)]

available_dates = sorted(schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d").unique(), reverse=True)
st.markdown("### ğŸ“… é–‹å‚¬æ—¥ã‚’é¸æŠ")
selected_date = st.selectbox("", available_dates)

# ã‚¦ãƒå¨˜é¸æŠ
st.markdown("### ğŸ‘§ ã‚¦ãƒå¨˜ã‚’é¸æŠ")
selected_umamusume = st.selectbox("", sorted(umamusume_df["kettou"]))
st.markdown("""
<div style='line-height: 1.5; font-size: 0,8em; color: gray;'>
ã‚ã„ã†ãˆãŠé †ã«ä¸¦ã‚“ã§ã„ã¾ã™ã€‚</div>
""", unsafe_allow_html=True)
target_kettou = name_to_kettou.get(selected_umamusume, "")
st.image(image_dict.get(selected_umamusume, ""), width=150)
st.markdown(f"é¸æŠã—ãŸã‚¦ãƒå¨˜ï¼š**{target_kettou}**")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨åˆ‡æ›¿
use_cache = st.radio("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨", ["ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†", "å†å–å¾—ã™ã‚‹"]) == "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†"
st.markdown("""
<div style='line-height: 1.5; font-size: 0,8em; color: gray;'>
ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæ¤œç´¢çµæœã®ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿ï¼‰ãŒã‚ã‚Œã°ã€çµæœã‚’ã™ãã«è¡¨ç¤ºã§ãã¾ã™ã€‚<br>
åŸºæœ¬çš„ã«ã€Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚<br><br>
ã€Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†ã€ã‚’é¸æŠã—ãŸå ´åˆã§ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã¯è‡ªå‹•ã§æƒ…å ±å–å¾—ãŒå§‹ã¾ã‚Šã¾ã™ã€‚<br>
æƒ…å ±å–å¾—ã«æ•°åç§’ï½æ•°åˆ†æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã®ã§ãŠå¾…ã¡ãã ã•ã„ã€‚<br>
</div>
""", unsafe_allow_html=True)

# æ¤œç´¢å®Ÿè¡Œ
selected_date_obj = pd.to_datetime(selected_date)
selected_rows = schedule_df[schedule_df["æ—¥ä»˜"] == selected_date_obj]
place_codes = {"æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04", "æ±äº¬": "05",
               "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08", "é˜ªç¥": "09", "å°å€‰": "10"}

status_text = st.empty()
total_races = len(selected_rows) * 12
all_race_counter = 0
overall_progress = st.progress(0)

if st.button("ğŸ” ã‚¦ãƒå¨˜è¡€çµ±ã‚µãƒ¼ãƒé–‹å§‹ï¼"):
    all_results = []
    full_cache = load_entire_cache()  # ğŸ” 1å›ã ã‘èª­ã¿è¾¼ã‚“ã§å…±æœ‰

    for _, row in selected_rows.iterrows():
        year = row["å¹´"]
        jj = place_codes.get(row["ç«¶é¦¬å ´"], "")
        kk = f"{int(row['é–‹å‚¬å›']):02d}"
        dd = f"{int(row['æ—¥ç›®']):02d}"
        place = row["ç«¶é¦¬å ´"]

        place_status = st.empty()
        place_status.markdown(f"### ğŸ“ {place} ç«¶é¦¬å ´ã®å‡ºèµ°é¦¬ã®å‡¦ç†ä¸­...")
        place_progress = st.progress(0)
        place_race_counter = 0
        place_results = []

        for nn in range(1, 13):  # 1Rã€œ12R
            race_id = f"{year}{jj}{kk}{dd}{nn:02d}"

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            if use_cache:
                cached = load_cached_result(race_id, target_kettou, full_cache=full_cache)
                if cached == "è©²å½“ãªã—":
                    place_race_counter += 1
                    all_race_counter += 1
                    place_progress.progress(min(place_race_counter / 12, 1.0))
                    overall_progress.progress(min(all_race_counter / total_races, 1.0))
                    continue
                elif cached:
                    all_results.extend(cached)
                    place_results.extend(cached)
                    place_race_counter += 1
                    all_race_counter += 1
                    place_progress.progress(min(place_race_counter / 12, 1.0))
                    overall_progress.progress(min(all_race_counter / total_races, 1.0))
                    continue

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            horse_links = get_horse_links(race_id)
            race_results = []

            for i, (name, link) in enumerate(horse_links.items(), 1):
                status_text.text(f"æ¤œç´¢ä¸­â€¦{place} {nn}R {i}/{len(horse_links)}é ­ç›®")
                try:
                    pedigree = get_pedigree_with_positions(link)
                    matched = match_pedigree(pedigree, target_kettou)
                    if matched:
                        result = {
                            "é¦¬å": name,
                            "è©²å½“ç®‡æ‰€": "ã€".join(matched),
                            "ç«¶é¦¬å ´": place,
                            "ãƒ¬ãƒ¼ã‚¹": f"{nn}R",
                            "ã‚¦ãƒå¨˜è¡€çµ±": target_kettou,
                            "race_id": race_id,
                        }
                        race_results.append(result)
                except Exception as e:
                    st.error(f"{name} ã®ç…§åˆã‚¨ãƒ©ãƒ¼ï¼š{e}")
                time.sleep(0.3)

            # çµæœä¿å­˜ï¼‹é€²æ—æ›´æ–°
            save_cached_result(race_results, race_id=race_id, bloodline=target_kettou)
            place_results.extend(race_results)
            all_results.extend(race_results)

            place_race_counter += 1
            all_race_counter += 1
            place_progress.progress(min(place_race_counter / 12, 1.0))
            overall_progress.progress(min(all_race_counter / total_races, 1.0))

        if place_results:
            st.markdown(f"### âœ… {place} ç«¶é¦¬å ´ã®è©²å½“é¦¬ä¸€è¦§")
            df = pd.DataFrame(place_results)
            st.markdown(render_table_html(df), unsafe_allow_html=True)

