import streamlit as st
import pandas as pd
import unicodedata
import time
from bs4 import BeautifulSoup
import requests
import gspread
import json
import os
from oauth2client.service_account import ServiceAccountCredentials

# === å®šæ•°è¨­å®š ===
HEADERS = {"User-Agent": "Mozilla/5.0"}
SPREADSHEET_ID = "1wMkpbOvqveVBkJSR85mpZcnKThYSEmusmsl710SaRKw"
SHEET_NAME = "cache_UMA"

# === Google Sheets æ¥ç¶š ===
def connect_to_gspread():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    json_str = os.environ.get("GOOGLE_CREDENTIALS_JSON")
    if not json_str:
        st.error("ç’°å¢ƒå¤‰æ•° GOOGLE_CREDENTIALS_JSON ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.stop()

    credentials_dict = json.loads(json_str)  # replace ãªã—ã§èª­ã¿è¾¼ã‚€
    credentials_dict["private_key"] = credentials_dict["private_key"].replace("\\n", "\n")  # â†ã“ã“ã ã‘å¤‰æ›

    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
    client = gspread.authorize(credentials)
    return client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

def load_cached_result(race_id, bloodline):
    sheet = connect_to_gspread()
    records = sheet.get_all_records()
    results = []
    for r in records:
        race_id_match = str(r.get("race_id", "")).strip() == str(race_id).strip()
        bloodline_match = str(r.get("ã‚¦ãƒå¨˜è¡€çµ±", "")).strip() == str(bloodline).strip()
        if race_id_match and bloodline_match:
            # è¡¨ç¤ºã«ä½¿ã‚ãªã„é …ç›®ã‚’é™¤å¤–
            filtered = {
                "é¦¬å": r.get("é¦¬å", ""),
                "è©²å½“ç®‡æ‰€": r.get("è©²å½“ç®‡æ‰€", ""),
                "ç«¶é¦¬å ´": r.get("ç«¶é¦¬å ´", ""),
                "ãƒ¬ãƒ¼ã‚¹": r.get("ãƒ¬ãƒ¼ã‚¹", "")
            }
            results.append(filtered)
    return results

def save_cached_result(rows):
    sheet = connect_to_gspread()
    headers = ["é¦¬å", "è©²å½“ç®‡æ‰€", "ç«¶é¦¬å ´", "ãƒ¬ãƒ¼ã‚¹", "ã‚¦ãƒå¨˜è¡€çµ±", "race_id"]
    values = [[row.get(h, "") for h in headers] for row in rows]
    sheet.append_rows(values)

# === è¡€çµ±ä½ç½®ãƒ©ãƒ™ãƒ« ===
def generate_position_labels():
    def dfs(pos, depth, max_depth):
        if depth > max_depth: return []
        result = [pos]
        result += dfs(pos + "çˆ¶", depth + 1, max_depth)
        result += dfs(pos + "æ¯", depth + 1, max_depth)
        return result
    return dfs("", 0, 5)[1:]
POSITION_LABELS = generate_position_labels()

# === ã‚¦ãƒå¨˜è¡€çµ±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===
umamusume_df = pd.read_csv("umamusume.csv")
image_dict = dict(zip(umamusume_df["kettou"], umamusume_df["url"]))
name_to_kettou = dict(zip(umamusume_df["kettou"], umamusume_df["kettou"]))

# === å‡ºèµ°é¦¬ãƒªãƒ³ã‚¯å–å¾— ===
def get_horse_links(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    res = requests.get(url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    horse_links = {}
    tables = soup.find_all("table", class_="RaceTable01")
    for table in tables:
        for a in table.find_all("a", href=True):
            if "/horse/" in a["href"]:
                name = a.get_text(strip=True)
                full_url = "https://db.netkeiba.com" + a["href"]
                if len(name) >= 2 and name not in horse_links:
                    horse_links[name] = full_url
    return horse_links

# === è¡€çµ±å–å¾— ===
def get_pedigree_with_positions(horse_url):
    horse_id = horse_url.rstrip("/").split("/")[-1]
    ped_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    res = requests.get(ped_url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.find("table", class_="blood_table")
    if not table:
        return {}
    names = {}
    td_list = table.find_all("td")
    for i, td in enumerate(td_list[:len(POSITION_LABELS)]):
        label = POSITION_LABELS[i]
        a = td.find("a")
        if a and a.text.strip():
            names[label] = a.text.strip()
    return names

# === ãƒãƒƒãƒåˆ¤å®š ===
def match_pedigree(pedigree_dict, target_name):
    matched_positions = []
    target_normalized = unicodedata.normalize("NFKC", target_name).strip().lower()
    for pos, name in pedigree_dict.items():
        key = unicodedata.normalize("NFKC", name).strip().lower()
        if key == target_normalized:
            matched_positions.append(pos)
    return matched_positions

# === HTMLãƒ†ãƒ¼ãƒ–ãƒ«å¤‰æ›ï¼ˆè¡¨ç¤ºé …ç›®ã‚’åˆ¶é™ï¼‰ ===
def render_table_html(df):
    df = df[["é¦¬å", "è©²å½“ç®‡æ‰€", "ç«¶é¦¬å ´", "ãƒ¬ãƒ¼ã‚¹"]]  # è¡¨ç¤ºç”¨ã«åˆ—ã‚’é™å®š
    df.insert(0, "No", range(1, len(df) + 1))  # Noåˆ—è¿½åŠ 
    table_html = "<table border='1' style='border-collapse:collapse; width:100%; font-size:16px;'>"
    table_html += "<thead><tr>" + "".join([f"<th>{col}</th>" for col in df.columns]) + "</tr></thead><tbody>"
    for _, row in df.iterrows():
        table_html += "<tr>" + "".join([f"<td>{row[col]}</td>" for col in df.columns]) + "</tr>"
    table_html += "</tbody></table>"
    return table_html

# === Streamlit UI ===
st.title("ğŸ‘§ ã‚¦ãƒå¨˜é€†å¼•ãè¡€çµ±ã‚µãƒ¼ãƒ")

# JRAæ—¥ä»˜
schedule_df = pd.read_csv("jra_2025_keibabook_schedule.csv")
schedule_df["æ—¥ä»˜"] = pd.to_datetime(
    schedule_df["å¹´"].astype(str) + "/" + schedule_df["æœˆæ—¥(æ›œæ—¥)"].str.extract(r"(\d{2}/\d{2})")[0],
    format="%Y/%m/%d"
)
today = pd.Timestamp.today()
past_31 = today - pd.Timedelta(days=31)
future_7 = today + pd.Timedelta(days=7)
schedule_df = schedule_df[schedule_df["æ—¥ä»˜"].between(past_31, future_7)]

available_dates = sorted(schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d").unique(), reverse=True)
selected_date = st.selectbox("ğŸ“… é–‹å‚¬æ—¥ã‚’é¸æŠ", available_dates)

# ã‚¦ãƒå¨˜é¸æŠ
selected_umamusume = st.selectbox("ğŸ‘§ ã‚¦ãƒå¨˜ã‚’é¸æŠ", sorted(umamusume_df["kettou"]))
target_kettou = name_to_kettou.get(selected_umamusume, "")
st.image(image_dict.get(selected_umamusume, ""), width=150)
st.markdown(f"é¸æŠã—ãŸã‚¦ãƒå¨˜ï¼š**{target_kettou}**")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨åˆ‡æ›¿
use_cache = st.radio("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨", ["ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†", "å†å–å¾—ã™ã‚‹"]) == "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†"

# æ¤œç´¢å®Ÿè¡Œ
selected_date_obj = pd.to_datetime(selected_date)
selected_rows = schedule_df[schedule_df["æ—¥ä»˜"] == selected_date_obj]
place_codes = {"æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04", "æ±äº¬": "05",
               "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08", "é˜ªç¥": "09", "å°å€‰": "10"}

status_text = st.empty()
total_races = len(selected_rows) * 12
all_race_counter = 0
overall_progress = st.progress(0)

if st.button("ğŸ” è©²å½“é¦¬ã‚’æ¤œç´¢"):
    for _, row in selected_rows.iterrows():
        year = row["å¹´"]
        jj = place_codes.get(row["ç«¶é¦¬å ´"], "")
        kk = f"{int(row['é–‹å‚¬å›']):02d}"
        dd = f"{int(row['æ—¥ç›®']):02d}"

        place_status = st.empty()
        place_status.markdown(f"### ğŸ“ {row['ç«¶é¦¬å ´']} ç«¶é¦¬å ´ã®å‡ºèµ°é¦¬ã®å‡¦ç†ä¸­...")
        place_progress = st.progress(0)
        place_race_counter = 0

        all_results = []
        place_results = []  # â† ç«¶é¦¬å ´ã”ã¨ã®çµæœã‚’ã“ã“ã«ã¾ã¨ã‚ã‚‹

        for race_num in range(1, 13):
            race_id = f"{year}{jj}{kk}{dd}{race_num:02d}"

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            if use_cache:
                cached = load_cached_result(race_id, target_kettou)
                if cached:
                    place_results.extend(cached)
                    all_race_counter += 1
                    place_race_counter += 1
                    place_progress.progress(min(place_race_counter / 12, 1.0))
                    overall_progress.progress(min(all_race_counter / total_races, 1.0))
                    continue

            horse_links = get_horse_links(race_id)
            race_results = []

            for i, (name, link) in enumerate(horse_links.items(), 1):
                status_text.text(f"æ¤œç´¢ä¸­â€¦{row['ç«¶é¦¬å ´']}{race_num}R {i}/{len(horse_links)}é ­ç›®")
                try:
                    pedigree = get_pedigree_with_positions(link)
                    matched = match_pedigree(pedigree, target_kettou)
                    if matched:
                        race_results.append({
                            "é¦¬å": name,
                            "è©²å½“ç®‡æ‰€": "ã€".join(matched),
                            "ç«¶é¦¬å ´": row["ç«¶é¦¬å ´"],
                            "ãƒ¬ãƒ¼ã‚¹": f"{race_num}R",
                            "ã‚¦ãƒå¨˜è¡€çµ±": target_kettou,
                            "race_id": race_id
                        })
                except Exception as e:
                    st.error(f"{name} ã®ç…§åˆã‚¨ãƒ©ãƒ¼ï¼š{e}")
                time.sleep(0.3)

            if race_results:
                df = pd.DataFrame(race_results)
                df_show = df[["é¦¬å", "è©²å½“ç®‡æ‰€", "ç«¶é¦¬å ´", "ãƒ¬ãƒ¼ã‚¹"]]  # è¡¨ç¤ºç”¨ï¼ˆ4åˆ—ï¼‰
                html = render_table_html(df_show)

                st.markdown(f"#### ğŸ¯ {row['ç«¶é¦¬å ´']} {race_num}R è©²å½“é¦¬", unsafe_allow_html=True)
                st.markdown(html, unsafe_allow_html=True)

                save_cached_result(race_results)
                place_results.extend(race_results)

            all_race_counter += 1
            place_race_counter += 1
            place_progress.progress(min(place_race_counter / 12, 1.0))
            overall_progress.progress(min(all_race_counter / total_races, 1.0))

        place_status.markdown(f"### âœ… {row['ç«¶é¦¬å ´']} ç«¶é¦¬å ´ã®å‡ºèµ°é¦¬ã®æŠ½å‡ºå®Œäº†")

        # === ç«¶é¦¬å ´ã”ã¨ã«ä¸€æ‹¬è¡¨ç¤º ===
        if place_results:
            st.markdown(f"### ğŸ‡ {row['ç«¶é¦¬å ´']} ç«¶é¦¬å ´ã®è©²å½“é¦¬ä¸€è¦§")
            df = pd.DataFrame(place_results)
            html = render_table_html(df)
            st.markdown(html, unsafe_allow_html=True)
